{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AmznDataLoader():\n",
    "        def __init__(self):\n",
    "#            df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "#            df = self.getDF('./data/reviews_Musical_Instruments_5.json.gz')\n",
    "    #         display(self.df)\n",
    "    #         print(self.df.loc[0][['overall','reviewText']])\n",
    "                df = self.getDF('./data/reviews_Amazon_Instant_Video_5.json.gz')\n",
    "    #            df = self.getDF('./data/reviews_Musical_Instruments_5.json.gz')\n",
    "                df = df[['reviewText', 'overall']]\n",
    "                df['reviewText'] = df['reviewText'].apply(lambda x : self.title_parsing(x))\n",
    "                X = df['reviewText']\n",
    "                y = df['overall']-1\n",
    "                self.weight_matrix = self.get_weight_matrix(X)\n",
    "                self.X_train = self.indicesMatrix(X)\n",
    "                self.y_train= y\n",
    "               # self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            \n",
    "            \n",
    "        def title_parsing(self, title):  \n",
    "            # remove stop words and tokenization \n",
    "            title = re.sub('[^a-zA-Z]', ' ', str(title))\n",
    "            title = title.lower()\n",
    "            title = remove_stopwords(title)  ## remove stop words, corpus size 52680            \n",
    "            title = title.split()\n",
    "            title = [word for word in title if len(word) >1 ]\n",
    "            return title\n",
    "        \n",
    "        \n",
    "        def parse(self, path):\n",
    "            g = gzip.open(path, 'rb')\n",
    "            for l in g:\n",
    "                yield eval(l)\n",
    "            \n",
    "        def getDF(self, path):\n",
    "            i = 0\n",
    "            df = {}\n",
    "            for d in self.parse(path):\n",
    "                df[i] = d\n",
    "                i += 1\n",
    "            return pd.DataFrame.from_dict(df, orient='index')        \n",
    "\n",
    "        def buildCorpus(self, X):\n",
    "            '''\n",
    "            return a dictionary with 'word' and its index in corpus as key and value respectively\n",
    "            '''\n",
    "            word2idx = {}\n",
    "            idx2word = [] ## alternatively use if.. condition\n",
    "            idx = 0 \n",
    "            for row in X:\n",
    "                for word in row:\n",
    "                    if word not in word2idx:\n",
    "                        idx2word.append(word)                \n",
    "                        word2idx[word] = len(idx2word) \n",
    "#             pickle.dump(word2idx, open('./data/corpusDict.txt', 'wb'))\n",
    "            return word2idx   \n",
    "        \n",
    "    \n",
    "        def indicesMatrix(self, X):\n",
    "            '''\n",
    "            return matrix (num_reviews, maxNumberWords) such that review text transformed to index\n",
    "            '''\n",
    "            word2idx = self.buildCorpus(X)\n",
    "#             word2idx = pickle.load(open('./data/corpusDict.txt', 'rb'))\n",
    "            ## 53008 words in corpus\n",
    "    \n",
    "            corpusSize = len(word2idx) \n",
    "        \n",
    "            ###\n",
    "            k = sorted(len(x) for x in X)\n",
    "            plt.hist(k)\n",
    "            ###\n",
    "            \n",
    "            \n",
    "#            maxNumberWords = sorted(len(x) for x in X)[-1]\n",
    "#            print (\"maximum\", maxNumberWords)'\n",
    "            maxNumberWords =60\n",
    "\n",
    "            index_matrix = np.zeros(( maxNumberWords,X.shape[0]))          \n",
    "            for i, row in enumerate(X):\n",
    "                for j, word in enumerate(row):\n",
    "#                     try:\n",
    "#                         index_matrix[i,j] = word2idx[word]\n",
    "#                         words_found += 1\n",
    "#                     except KeyError:\n",
    "#                         index_matrix[i,j] = corpusSize     \n",
    "\n",
    "                    index_matrix[j,i] = word2idx[word]\n",
    "                    if j >= maxNumberWords -1: \n",
    "                        break\n",
    "#            if maxNumberWords % 2 == 1:\n",
    "#                x0 = np.full((index_matrix.shape[0], 1), maxNumberWords)\n",
    "#                index_matrix = np.hstack((index_matrix, x0))\n",
    "            print(index_matrix.shape)\n",
    "            return index_matrix\n",
    "        \n",
    "        def get_weight_matrix(self, X): #max norm for linear layer\n",
    "            '''\n",
    "            return matrix contains embedding for word in corpus/review text\n",
    "            Note that the word cannot be found in the glove returns ?? as embedding\n",
    "            '''\n",
    "\n",
    "            glove = {}\n",
    "\n",
    "            with open(f'./data/glove.6B.50d.txt', 'rb') as f:#\n",
    "                for l in f:\n",
    "                    line = l.decode().split()           \n",
    "                    word = line[0]\n",
    "        #            words.append(word)\n",
    "        #             word2idx[word] = idx\n",
    "        #            idx += 1\n",
    "                    vect = np.array(line[1:]).astype(np.float)\n",
    "        #             vectors.append(vect)\n",
    "        #     vectors = np.reshape(vectors, (400000, 50))\n",
    "                    glove.update({word:vect})\n",
    "        #     glove = {word2idx[w]: vectors[word2idx[w]] for w in words}  # alternatively generate weights_matrix directly\n",
    "\n",
    "            target_vocab = self.buildCorpus(X)\n",
    "            #except\n",
    "            #exceptKey = list(set(list(glove.keys())).difference(list(target_vocab.keys())))  ## \n",
    "            matrix_len = len(target_vocab)\n",
    "            weights_matrix = np.zeros((50,matrix_len+1))#\n",
    "            words_found = 0\n",
    "            words_not_found = 0\n",
    "            for i, word in enumerate(target_vocab):\n",
    "                try: \n",
    "                    weights_matrix[:,i+1] = glove[word]\n",
    "                    words_found += 1\n",
    "                except KeyError:\n",
    "                    words_not_found += 1\n",
    "                    weights_matrix[:,i+1] = np.random.normal(scale=0.6, size=(50,))#\n",
    "\n",
    "            print(words_not_found)\n",
    "            print(\"w\",weights_matrix.shape)\n",
    "            return  weights_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9776\n",
      "w (50, 52657)\n",
      "(60, 37126)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = AmznDataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7834814814814814"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(data_loader.X_train)/(data_loader.X_train.shape[0]*data_loader.X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader.X_train.shape\n",
    "# data_loader.buildCorpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.102264404296875\n"
     ]
    }
   ],
   "source": [
    "weight_matrix = data_loader.weight_matrix\n",
    "import sys\n",
    "print(sys.getsizeof(weight_matrix)/(2**20))\n",
    "\n",
    "# print(weight_matrix[0])\n",
    "# print(weight_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(weight_matrix,  open('./data/GloveMatrix.npy', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(data_loader.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.0, 67.49512670565302),\n",
       " (2.0, 7.69980506822612),\n",
       " (3.0, 20.382553606237817),\n",
       " (0.0, 2.022417153996101),\n",
       " (1.0, 2.4000974658869394)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, c[i]/len(data_loader.y_train)*100) for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = Variable(torch.cuda.FloatTensor([0.4,0.5,0.11,0.23,0.56])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class ReviewModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(ReviewModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix.T\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embed_size = weights_matrix.shape[0]\n",
    "        \n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        self.conv1 = nn.Conv1d(in_channels=self.embed_size, out_channels=200, kernel_size=4, padding=2).double()#\n",
    "        self.conv2 = nn.Conv1d(in_channels=self.embed_size, out_channels=200, kernel_size=5, padding=2).double()#\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        self.drop2 = nn.Dropout(p=0.3)\n",
    "        self.rnn = nn.GRU(input_size=400, hidden_size=100, num_layers=1, batch_first=False) \n",
    "        self.fc1 =nn.Linear(in_features=(max_length//2)*100, out_features = 400)\n",
    "        max_norm(self.fc1)\n",
    "        self.drop3 = nn.Dropout(p=0.15)\n",
    "        self.fc2 = nn.Linear(in_features=400, out_features=5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x_size = (batch_size, max_seq_len)\n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        # x_size = (batch_size, max_seq_len, embed_size)\n",
    "        x = self.drop1(x)\n",
    "        print(x.size())\n",
    "        #x = x.view(-1,self.embed_size,self.max_length)  \n",
    "        # x_size = (batch_size,embed_size,max_seq_len) \n",
    "        \n",
    "        x1 = F.relu(self.conv1(x))\n",
    "        \n",
    "        x2 = F.relu(self.conv2(x))\n",
    "        # x1_size = (batch_size, 200, max_seq_len+1)\n",
    "        # x2_size = (batch_size, 200, max_seq_len)\n",
    "\n",
    "        x1 = self.maxpool(x1)\n",
    "        \n",
    "        x2 = self.maxpool(x2)\n",
    "        # x1_size = x2_size = (batch_size,200, max_seq_len//2)\n",
    "\n",
    "        x = torch.cat((x1,x2), 2)\n",
    "        # x_size = (batch_size, 400, max_seq_len//2)\n",
    "        print(\"cat\",x.size())\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        # x = x.view((self.max_length//2),-1, 400).float()\n",
    "        # x_size = (max_seq_len//2, batch_size, 400)\n",
    "\n",
    "        hidden = Variable(torch.cuda.FloatTensor(1, batch_size, 100).uniform_()) \n",
    "        output, _ = self.rnn(x,hidden)\n",
    "        # output_size = (max_seq_len//2, batch_size, hidden_size)\n",
    "\n",
    "        x = output.contiguous().view(self.batch_size,-1)\n",
    "        # x_size = (batch_size, max_seq_len//2 *hidden_size)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x_size = (batch_size, 400)\n",
    "        \n",
    "        x = self.drop3(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        # x_size = (batch_size, out_dim)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo reset parameters for linear layer\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = X.shape[0](torch.tensor(weights_matrix), freeze=False)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=100, \n",
    "                                              kernel_size=(fs,50)).double() for fs in [2,3,4]])\n",
    "        self.fc = nn.Linear(3*100,2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.batch_size)\n",
    "        x = x.permute(1,0)\n",
    "#         print(\"input 1tensor\", x.size())\n",
    "        x = self.embedding(x)\n",
    "        x = x.unsqueeze(1)\n",
    "        conved = [F.relu(conv(x)).squeeze(3) for conv in self.convs]\n",
    "        x = [F.max_pool1d(conv,conv.shape[2]).squeeze(2).float() for conv in conved]\n",
    "        x = self.dropout(torch.cat(x, dim = 1))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm(model, max_val=3, eps=1e-8):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'bias' not in name:\n",
    "            norm = param.norm(2, dim=0, keepdim=True)\n",
    "            desired = torch.clamp(norm, 0, max_val)\n",
    "            param = param * (desired / (eps + norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-dae5a8411da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#model = DynamicModel(max_length = max_length, batch_size = batch_size, parameter_dict=SST2_DATASET_PARAMETERS).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#model = SimpleModel(max_length, batch_size = batch_size).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReviewModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# net = NeuralNetClassifier(model,## change dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_backward_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;31m# Tensors stored in modules are graph leaves, and we don't\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m                 \u001b[0;31m# want to create copy nodes, so we have to unpack the data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/work/modules/Ubuntu/14.04/amd64/common/anaconda3/latest/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    ('es1',EpochScoring('accuracy')),\n",
    "    ('lrs',LRScheduler()),\n",
    "    ('est',EarlyStopping()),\n",
    "    \n",
    "]\n",
    "\n",
    "            \n",
    "batch_size = 128\n",
    "device = torch.device(\"cuda\")\n",
    "max_length = 60\n",
    "#model = DynamicModel(max_length = max_length, batch_size = batch_size, parameter_dict=SST2_DATASET_PARAMETERS).to(device)\n",
    "#model = SimpleModel(max_length, batch_size = batch_size).to(device)\n",
    "model = ReviewModel(max_length, batch_size).to(device)\n",
    "\n",
    "# net = NeuralNetClassifier(model,## change dimensionality\n",
    "#                           iterator_train__drop_last = True,\n",
    "#                           iterator_valid__drop_last = True, \n",
    "#                           iterator_train__shuffle = True,\n",
    "#                           iterator_valid__shuffle = True,\n",
    "#                           max_epochs=100, \n",
    "#                           criterion = nn.CrossEntropyLoss, \n",
    "#                           #criterion__weight = weight, \n",
    "#                           optimizer=optim.Adagrad,\n",
    "#                           lr = 0.01,\n",
    "#                           #optimizer_param_groups = [('momentum',0.5)],\n",
    "#                           #optimizer__param_groups =[('eps', 1e-6)],  ## review model , sgd, lr = 0.01, m = 0,5\n",
    "#                           batch_size = batch_size,\n",
    "#                           callbacks = callbacks,\n",
    "#                           device = device,verbose = 1\n",
    "#                           )\n",
    "\n",
    "net = NeuralNetClassifier(model,## change dimensionality\n",
    "                          iterator_train__drop_last = True,\n",
    "                          iterator_valid__drop_last = True, \n",
    "                          iterator_train__shuffle = True,\n",
    "                          iterator_valid__shuffle = True,\n",
    "                          max_epochs=100, \n",
    "                          criterion = nn.CroessEntropyLoss, \n",
    "                          #criterion__weight = weight, \n",
    "                          optimizer=optim.Adagrad,\n",
    "                          lr = 0.01,\n",
    "                          #optimizer_param_groups = [('eps',1e-6)],\n",
    "                          #optimizer__param_groups =[('eps', 1e-6)],  ## review model , sgd, lr = 0.01, m = 0,5\n",
    "                          batch_size = batch_size,\n",
    "                          callbacks = callbacks,\n",
    "                          device = device,verbose = 1\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-189-99b258957d64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# rest = inputs.size()[0] % batch_sizen_filters]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# print(rest)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# print(inputs.size())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "inputs = torch.from_numpy(data_loader.X_train).long().to(device)\n",
    "labels = torch.tensor(data_loader.y_train.values).long().to(device)\n",
    "# rest = inputs.size()[0] % batch_sizen_filters]\n",
    "# print(rest)\n",
    "# print(inputs.size())\n",
    "# inputs = inputs[:-rest]\n",
    "# print(inputs.size()[0]/batch_size)\n",
    "# print(inputs.shape)\n",
    "# labels = labels[:-rest]\n",
    "net.fit(inputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 37126)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_loader.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model\n",
    "with open(\"model.pkl\", 'wb') as f:\n",
    "    pickle.dump(\"model\",f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"model.pkl\",\"rb\") as f: \n",
    "    model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = data_loader.indicesMatrix()\n",
    "import sklearn\n",
    "sorted(sklearn.metrics.SCORERS.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_losses(tr_loss, val_loss):\n",
    "    plt.plot(tr_loss, label=\"training\")\n",
    "    plt.plot(val_loss, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo plot losses\n",
    "#todo plot accuracy\n",
    "#todo confusion matrix\n",
    "history = net.history\n",
    "train_losses = history[:, 'train_loss']\n",
    "valid_losses = history[:, 'valid_loss']\n",
    "\n",
    "accuracy = history[:, 'accuracy']\n",
    "plot_losses(train_losses, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'lr': [0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9],\n",
    "#     'max_epochs': list(range(10,100,20))\n",
    "# }\n",
    "# gs = GridSearchCV(net, params, refit = False, scoring = 'accuracy',verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(data_loader.y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4.0, 67.49512670565302),\n",
       " (2.0, 7.69980506822612),\n",
       " (3.0, 20.382553606237817),\n",
       " (0.0, 2.022417153996101),\n",
       " (1.0, 2.4000974658869394)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, c[i]/len(data_loader.y_train)*100) for i in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = Variable(torch.cuda.FloatTensor([0.4,0.5,0.11,0.23,0.56])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.callbacks import EpochScoring, PrintLog, ProgressBar, LRScheduler, EarlyStopping\n",
    "from sklearn.metrics import accuracy_score,make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "SST2_DATASET_PARAMETERS = {\n",
    "    \"cell_one_parameter_dict\" : {\n",
    "        \"sent_length\": 19,\n",
    "        \"conv_kernel_size\": (7, 1),\n",
    "        \"conv_input_channels\": 1,\n",
    "        \"conv_output_channels\": 6,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 10,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"cell_two_parameter_dict\" : {\n",
    "        \"sent_length\": None,\n",
    "        \"conv_kernel_size\": (5, 1),\n",
    "        \"conv_input_channels\": 6,\n",
    "        \"conv_output_channels\": 14,\n",
    "        \"conv_stride\": (1, 1),\n",
    "        \"k_max_number\": 4,\n",
    "        \"folding_kernel_size\": (1, 2),\n",
    "        \"folding_stride\": (1, 2)\n",
    "    },\n",
    "    \"dropout_rate\": 0.5,\n",
    "    \"embedding_dim\": 50,\n",
    "    \"vocab_length\": data_loader.weight_matrix,\n",
    "    \"output_dim\": 2\n",
    "}\n",
    "SST2_DATASET_PARAMETERS[\"cell_two_parameter_dict\"][\"sent_length\"] = SST2_DATASET_PARAMETERS[\"cell_one_parameter_dict\"][\"k_max_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# todo reset parameters for linear layer\n",
    "class DynamicModel(nn.Module):\n",
    "    def __init__(self, max_length, batch_size,parameter_dict):\n",
    "        super().__init__()\n",
    "        self.parameter_dict = parameter_dict\n",
    "        weights_matrix = data_loader.weight_matrix\n",
    "        self.max_length = max_length\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding = nn.Embedding.from_pretrained(torch.tensor(weights_matrix), freeze=False)\n",
    "        self.dcnn_first_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_one_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_one_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_one_parameter_dict\"][\"folding_stride\"],\n",
    "        ).double()\n",
    "        self.dcnn_last_cell = DCNNCell(\n",
    "            cell_number=-1,\n",
    "            sent_length=self.parameter_dict[\"cell_two_parameter_dict\"][\"sent_length\"],\n",
    "            conv_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_kernel_size\"],\n",
    "            conv_input_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_input_channels\"],\n",
    "            conv_output_channels=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"],\n",
    "            conv_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_stride\"],\n",
    "            k_max_number=self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"],\n",
    "            folding_kernel_size=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_kernel_size\"],\n",
    "            folding_stride=self.parameter_dict[\"cell_two_parameter_dict\"][\"folding_stride\"],\n",
    "        )  .double()      \n",
    "        self.fc_layer_input = self.parameter_dict[\"cell_two_parameter_dict\"][\"k_max_number\"] *\\\n",
    "            self.parameter_dict[\"cell_two_parameter_dict\"][\"conv_output_channels\"] *\\\n",
    "            math.floor(self.parameter_dict[\"embedding_dim\"]/4)\n",
    "            \n",
    "        self.dropout = nn.Dropout(self.parameter_dict[\"dropout_rate\"])\n",
    "        self.flatten = Flatten()\n",
    "        self.fc = nn.Linear(self.fc_layer_input, self.parameter_dict[\"output_dim\"])\n",
    "        \n",
    "    def forward(self, inp):\n",
    "        # [batch_size, sent_length]\n",
    "        embedded = self.embedding(inp)\n",
    "        #print(embedded.size())\n",
    "        # [batch_size, sent_length, embedding_dim]\n",
    "        # adding single channel dimension\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        # print(embedded.shape)\n",
    "        # [batch_size, 1(initial_input_channel), sent_length, embedding_dim]\n",
    "        out = torch.tanh(self.dcnn_first_cell(embedded))\n",
    "        # print(out.shape)\n",
    "        # [batch_size, first_cell_output_channels, first_cell_k_maxed_number, embedding_dim]\n",
    "        out = torch.tanh(self.dcnn_last_cell(out))\n",
    "        # print(out.shape)\n",
    "        # [batch_size, last_cell_output_channels, last_cell_k_maxed_number, embedding_dim/2]\n",
    "        out = self.dropout(self.flatten(out)).float()\n",
    "        # print(flat.shape)\n",
    "        #[batch_size, last_cell_output_channels * last_cell_k_maxed_number * embedding_dim/2]\n",
    "        out = self.fc(out)\n",
    "        # print(fc.shape)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/reviews_Amazon_Instant_Video_5.json.gz'\n",
    "\".txt\" in path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCNNCell(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        cell_number=1,\n",
    "        sent_length=7,\n",
    "        conv_kernel_size=(3, 1),\n",
    "        conv_input_channels=1,\n",
    "        conv_output_channels=2,\n",
    "        conv_stride=(1, 1),\n",
    "        k_max_number=5,\n",
    "        folding_kernel_size=(1, 2),\n",
    "        folding_stride=(1,1)\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.cell_number=cell_number \n",
    "        self.sent_length=sent_length\n",
    "        self.conv_kernel_size=conv_kernel_size\n",
    "        self.conv_input_channels=conv_input_channels\n",
    "        self.conv_output_channels=conv_output_channels\n",
    "        self.conv_stride=conv_stride\n",
    "        self.k_max_number=k_max_number\n",
    "        self.folding_kernel_size=folding_kernel_size\n",
    "        self.folding_stride=folding_stride\n",
    "        \n",
    "        # calculating padding size\n",
    "        self.pad_0_direction = math.ceil(self.conv_kernel_size[0] - 1)\n",
    "        self.pad_1_direction = math.ceil(self.conv_kernel_size[1] - 1)\n",
    "        \n",
    "        # 2d convolution\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels=self.conv_input_channels,\n",
    "            out_channels=self.conv_output_channels,\n",
    "            kernel_size=self.conv_kernel_size,\n",
    "            stride=self.conv_stride,\n",
    "            padding=(self.pad_0_direction, self.pad_1_direction)\n",
    "        )\n",
    "        \n",
    "        # if cell is last then initialising folding\n",
    "        if cell_number == -1:\n",
    "            self.fold = nn.AvgPool2d(kernel_size=self.folding_kernel_siz§e, stride=self.folding_stride)\n",
    "            \n",
    "    def forward(self, inp):\n",
    "        \n",
    "        # [batch_size, input_channels, sent_length_in, embedding_dim]\n",
    "        conved = self.conv_layer(inp)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length_out, embedding_dim]\n",
    "        if self.cell_number == -1:\n",
    "            conved = self.fold(conved)\n",
    "        \n",
    "        # [batch_size, out_channels, sent_length, embedding_dim/2]\n",
    "        k_maxed = torch.topk(conved, self.k_max_number, dim=2, largest=True)[0]\n",
    "        \n",
    "        # [batch_size, out_channels, k_maxed_number, embedding_dim/2]\n",
    "        return k_maxed\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'./data/reviews_Amazon_Instant_Video_5.json.gz'\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
